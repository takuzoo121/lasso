<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>A COURSE IN LARGE SAMPLE THEORY</title>
    <link href="main.css" rel="stylesheet">
    <!--<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script type="text/javascript" src="main.js"></script>
    <!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
    <!--<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>-->
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>  </head>
  <body>
    <!--<header>
      <a href="1page.html#thema">前ページ</a>
      <a href="#abs">概要</a>
      <a href="#theorem1">1.中心極限定理</a>
      <a href="#theorem2">2.例</a>
      <a href="#theorem3">3.近似の改善</a>
    </header>-->
    <div class="container markdown-body">
      <h1 id="thema">第16章 Sparse Modeling and the Lasso </h1>

      <div class="abstract border">
        <h2 id="abs">概要</h2>
        <p>0. Introduction</p>
        <p class="indent">この章の内容</p>
        <p>1. Forward Stepwise Regression</p>
        <p class="indent">変数選択のアルゴリズム</p>
        <p>2. The Lasso</p>
        <p class="indent">Lassoの説明</p>
        <p>3. Fitting Lasso Models</p>
        <p class="indent">Lassoモデルの解き方</p>
        <p>4. Least-Angle Regression</p>
        <p class="indent">Lassoの解の求め方</p>
        <p class="indent">4.1 Lasso and Degrees of Freedom</p>
        <p>5. Fitting Generalized Lasso Models</p>
        <p class="indent">最小二乗以外の損失関数について</p>
        <p>6. Post-Selection Inference for the Lasso</p>
        <p class="indent">Lassoモデルの解釈性</p>
        <p>7. Connections and Extensions</p>
        <p class="indent">他の予測問題の手法との繋がり</p>
        <p class="indent">7.1 Lasso Logistic Regression and the SVM</p>
        <p class="indent">7.2 Lasso and Boosting</p>
        <p class=indent>7.3 Extensions of the Lasso</p>
      </div>

      <div class="contexts">
        <h2 id="intro">0. Introduction</h2>
        <p>この章ではwide data setsを対象とする。</p>
        <p>wide dataとは、</p>
        <p>変数の数がサンプル数をはるかに超えるデータのこと。</p>

        <p>例</p>
        <p>辞書のやつ</p>
        <p>\(\Downarrow\)</p>
        <p>数百の説明変数を回帰や分類に使うなら、全ての説明変数は使いたくない。</p>
        <p>なぜなら、冗長な変数を用いると汎化性能下がる。</p>
        <p>\(\Downarrow\)</p>
        <p>そこで良いサブセットの選択(=変数選択)の手法について興味がある。</p>
        <p>この章では</p>
        <p>1. 変数選択(特にforward-stepwiseアプローチについて)</p>
        <p>2. Lasso</p>
        <p>3. LARsアルゴリズム(Lassoのsolution-pathを求める)</p>
        <p>4. big dataとwide dataへの他のアプローチ</p>
        <p>の大きく分けて4つについて説明する。</p>

      </div>

      <div class="contexts">
        <h2 id="section1">1. Forward Stepwise Regression</h2>
        <!--<p>??stepwise変数選択法は長い歴史があり、元々はデータセットのサイズが</p>
        <p>非常に控えめなとき、特に変数の数に関して条件で考案された。</p>-->
        <p>stepwise変数選択法は最良サブセット選択法から派生した手法で、</p>
        <p>性能は悪い代わりに、計算コストは低い。</p>
        <p>ここではまず、最良サブセット選択法について説明する。</p>

        <h3>1.1 best subset regression</h3>
        <h4>\(\ \ \)仮定</h4>
        <p>サンプル数 : n</p>
        <p>応答 : \(y_i(i=1,\cdots,n)\)</p>
        <p>説明変数 : \(\bf{x}_{\rm{i}}^{\rm{T}}=\rm{(x_{i1},\cdots,x_{ip})}\)</p>
        <p>(この章で扱うモデル : 線形回帰モデルorロジスティック回帰モデル)</p>
        <p>(損失関数 : L(二乗和or対数尤度))</p>

        <div class="border">
          <h4>アルゴリズム16.1 best subset regression</h4>
          <p>1. ステップ1</p>
          <p class="indent">m=0から始める。</p>
          <p class="indent">\(\hat{\eta_{0}}(\bf{x}\rm{)} = \rm{\hat{\beta_{0}}}\)とする。
           ただし、\(\hat{\beta_{0}}\)は応答yの平均である。</p>
          <p>2. ステップ2</p>
          <p class="indent">m=1では、応答yに最もfitする1つの変数\(x_j\)を求める。</p>
          <p class="indent">fit度合いは、線形回帰モデルとして\(\hat{\eta}_{1}(x_j) = \hat{\beta}_0 + x_j \hat{\beta}_j\)
            をおき、</p>
          <p class="indent">損失関数L(\(y, \hat{\eta}_1(x_j)\))が最小となる説明変数\(x_j\)を求める。</p>
          <p class="indent">そして、サブセット\(A_1 = \{j\}\)とおく。 </p>
          <p>3. ステップ3</p>
          <p class="indent">各サブセットサイズ\(m\in \{2,3,\cdots,M\}\)で最適なサブセット\(A_m\)</p>
          <p class="indent">を求める。ただし、\(M \leq \min(n-1, p)\)である。</p>
          <p class="indent">ここでも同様に、線形回帰モデル\(\hat{\eta}_{m}(x_{A_m}) = \hat{\beta}_0 + x_{A_m}^{T} \hat{\beta}_{A_m}\)
          をおき、</p>
          <p class="indent">損失関数L(\(y, \hat{\eta}_m(x_{A_m})\))が最小となる説明変数のサブセットを求める。</p>
          <p>4. ステップ4</p>
          <p class="indent">これらのM個のサブセットから最良のサブセットを決める。</p>
        </div>
        <p></p>
        <p>ステップ4で最適なサブセットを選択するためにいくつかの手法がある。</p>
        <p>ここでは、K-fold cross-validationを紹介する。</p>
        <div class="border">
          <h4>K-fold cross-validation</h4>
          <p>1. CVとは</p>
          <p class="indent">データを訓練用とテスト用に分けて性能を検証する方法。</p>
          <p>2. K-fold法とは</p>
          <p>\(\ \ \)2.1 データをサイズが等しくなるようにK個にランダムに分ける。</p>
          <p>\(\ \ \)2.2 K-1のデータを訓練データとし、残り1つをテストデータとする。</p>
          <p>\(\ \ \)2.3 これをK通りすべてで試す。</p>
          <p>\(\ \ \)2.4 サブセットサイズ\(m(=1,\cdots,M)\)でのK回の性能の平均を取る。</p>
          <p>\(\ \ \)2.5 最良の性能を誇るサブセット\(A_{m'}\)を選ぶ。</p>
        </div>
        <p>※</p>
        <p>最適なサブセット\(A_{m'}\)はK回の実験で異なるサブセットを</p>
        <p>選んでいる可能性がある。</p>
        <p>\(\Downarrow\)</p>
        <p>求まった最適なサブセットサイズ\(m'\)で、</p>
        <p>全てのサンプルを用いてもう一度変数選択する。</p>

        <h3>1.2 forward stepwise regression</h3>
        <p>上の最良サブセット回帰には問題がある。</p>
        <p>それはステップ3での計算コストが大きいことである。</p>
        <p>\(\Downarrow\)</p>
        <p>その問題解決のためにstepwise変数選択法が提案された。</p>
        <p>そのstepwise変数選択法の1種であるforward stepwise regressionの</p>
        <p>アルゴリズムを以下で説明する。</p>
        <div class="border">
          <h4>アルゴリズム16.2 forward stepwise regression</h4>
          <p>1. ステップ1</p>
          <p class="indent">m=0から始める。</p>
          <p class="indent">\(\hat{\eta_{0}}(\bf{x}\rm{)} = \rm{\hat{\beta_{0}}}\)とする。
           ただし、\(\hat{\beta_{0}}\)は応答yの平均である。</p>
          <p>2. ステップ2</p>
          <p class="indent">m=1では、応答yに最もfitする1つの変数\(x_j\)を求める。</p>
          <p class="indent">fit度合いは、線形回帰モデルとして\(\hat{\eta}_{1}(x_j) = \hat{\beta}_0 + x_j \hat{\beta}_j\)
            をおき、</p>
          <p class="indent">損失関数L(\(y, \hat{\eta}_1(x_j)\))が最小となる説明変数\(x_j\)を求める。</p>
          <p class="indent">そして、サブセット\(A_1 = \{j\}\)とおく。 </p>
          <p>3. <font color="red">ステップ3</font>(ここだけ変わる)</p>
          <p class="indent">各サブセットサイズ\(m\in \{2,3,\cdots,M\}\)で最適なサブセット\(A_m\)</p>
          <p class="indent">を求める。ただし、\(M \leq \min(n-1, p)\)である。</p>
          <p class="indent">ここでも同様に、線形回帰モデル\(\hat{\eta}_{m}(x_{A_m}) = \hat{\beta}_0 + x_{A_m}^{T} \hat{\beta}_{A_m}\)
          をおき、</p>
          <p class="indent">損失関数L(\(y, \hat{\eta}_m(x_{A_m})\))が最小となる説明変数のサブセットを求める。</p>
          <p class="indent">そして、サブセット\(A_1,\cdots,A_M\)は<font color="red">nest</font>する。</p>
          <p>4. ステップ4</p>
          <p class="indent">これらのM個のサブセットから最良なサブセットを決める。</p>
        </div>
        <p>※</p>
        <p>サブセットがnestしているとは、</p>
        <p>\(A_1 \subset A_2 \subset \cdots \subset A_M\)が成り立つことを示す。</p>
        <p>これより、サブセットの大きさが1つ大きくなるたびに1つだけ変数を</p>
        <p>選択すれば良いことになる。</p>

        <h4>\(\ \ \)例.</h4>
        <p>spamデータでforward stepwise regressionを行う。</p>
        <p>説明変数 : 57の単語</p>
        <p>訓練データ数 : 3065</p>
        <p>testデータ : 1536</p>
        <p>応答 : +1(スパム) or -1(notスパム)</p>
        <p>詳細は表8.3, 図8.7, 図12.2らへん見て。</p>
        <p>その結果を以下に図で示す。</p>

        <div align="center">
          <figure id="fig1">
            <img src="fig1.png" width="300" height="200">
            <figcaption>図16.1</figcaption>
          </figure>
        </div>

        <p>ここで横軸の\(R^2\)は決定係数(=寄与率)であり、</p>
        <!--<p>説明変数が被説明変数をどのくらい説明できているかを表す値である。</p>-->
        <p>そして縦軸は係数(\(\beta_0, \cdots, \beta_p\))の大きさを表す。</p>
        <p>また灰色の線は各ステップを表す。</p>
        <p>nestしてるので、1度非0になった係数はずっと非0である。</p>

        <!---<p>forward stepwise regressionを行うと、</p>
        <p>(n>pの場合)全ての変数に対して1つの最小二乗fitと同じコストを持つ。</p>
        <p>これは変数の追加のたびにモデルの列が更新されるからです。</p>
        <p>しかし、これは線形モデルと二乗誤差損失の結果です。</p>-->

        <h3>1.3 forward stepwise logistic regression</h3>
        <p>ここではforward stepwise ロジスティック回帰を行うと仮定する。</p>
        <!--<p>ここで更新は機能せず、全てのfit度は変数が加わるたびに最尤最尤法で計算される。</p>-->
        <p>ステップ3で追加する変数を特定するには、</p>
        <p>原則として(m+1)変数モデルで(p-m)回適合度を求め、</p>
        <p>最も尤もらしい変数を見つける。</p>
        <p>実際に適合度を測るときは、計算コストの低いスコアテストがある。</p>
        <!--<p>例えば、m項でモデルをフィッティングするための反復的に</p>
        <p>重み付けされた最小二乗（IRLS）反復からの対数尤度に対する</p>
        <p>二次近似を用いることに相当する。</p>-->
        <!--<p>モデルにない変数のスコアテストは、この変数を加重最小二乗フィットに含めるテストと同じです。</p>-->
        <h4>\(\ \)例</h4>
        <p>forward stepwise ロジスティック回帰を実際のデータで使ってみる。</p>
        <h4>\(\ \)図16.2</h4>
        <p>横軸ステップ数(各ステップで使う変数1つ増える)、</p>
        <p>縦軸誤分類率としてspamデータに対する</p>
        <p>forward stepwise 線形回帰とforward stepwise ロジスティック回帰の</p>
        <p>性能を比較する。</p>
        <p>ただし、ここで線形回帰は、出力が0より大きいと+1、小さいと-1とする。</p>
        <div align="center">
          <figure id="fig2">
            <img src="fig2.png" width="300" height="200">
            <figcaption>図16.2</figcaption>
          </figure>
        </div>
        <p>ロジスティック回帰の方が性能良い。</p>
        <p>\(\Downarrow\)</p>
        <!--<p>forward stepwise変数選択法は大きいpで使えるが、冗長になる。</p>
        <p>しかし、理想的なactive setがかなり小さい場合は数千もの変数があっても実行可能な選択だ。</p>-->
        <!--<p>forward stepwise変数選択は、大きいp(数千とか)でも使えるがあまり使わない。</p>
        <p>ただ、pが大きく理想的なactive setがかなり小さい場合は使われる。</p>-->
        <p>forward stepwise変数選択法は、図16.2に示すように</p>
        <p>一連のモデル(用いる変数の数が異なるモデル)を提供する。</p>
        <p>1つのモデルを選択するためにCVをよく用いる。</p>
        <p>\(\Downarrow\)</p>
        <h4>\(\ \)図16.3</h4>
        <p>spamデータとstepwise線形回帰を用いて試してみる。</p>
        <p>ここで横軸はステップ数で、縦軸は誤分類率である。</p>
        <div align="center">
          <figure id="fig3">
            <img src="fig3.png" width="300" height="200">
            <figcaption>図16.3</figcaption>
          </figure>
        </div>
        <p>ここで、一連のモデルは2値応答に対する2乗誤差損失を用いて適合される。</p>
        <p>しかし、CVはこのモデリングの最終目的の誤分類エラーを用いて</p>
        <p>各モデルを評価する。</p>
        <p>なぜなら、この問題の最終目標は誤分類エラーを減らすことだが、</p>
        <p>パラメータ推定に関して、誤分類エラーは不連続で微分不可能なので</p>
        <p>損失関数としては扱いにくいからだ。</p>
        <!--<p>これは、CVの利点の1つを示している。</p>
        <p>便利な(微分可能で滑らかな)損失関数が一連のモデルの適合に使われる。</p>
        <p>しかし、我々は一連のモデルの評価に任意の指標が使える。</p>
        <p>ここでは、誤分類エラーが使われる。</p>-->
        <!--<p>そして、線形モデルパラメータに関して、誤分類エラーはパラメータ推定に使うには</p>
        <p>難しく不連続な損失関数となる。</p>
        <p>ここで、誤分類エラーを用いた理由は最適なモデルサイズを選ぶためである。</p>-->
        <p>よって、2乗誤差損失を小さくしても誤分類エラーについては</p>
        <p>25ステップを超えて得られる恩恵はない。</p>


      </div>

      <div class="contexts">
        <h2 id="section2">2. The Lasso</h2>
        <p>stepwise変数選択法の問題点</p>
        <p>1. 問題点1</p>
        <p class="indent">stepwise変数選択法はpが大きく、</p>
        <p class="indent">理想的なactive setのサイズが小さい場合には役立つ。</p>
        <p class="indent">しかし適度な数の変数が役割を果たす場合、扱いにくくなる。</p>
        <p>2. 問題点2</p>
        <p class="indent">forward stepwise変数選択法は貪欲法である。</p>
        <p class="indent">貪欲法とは</p>
        <p class="indent">問題の要素を複数の部分問題に分割し、それぞれを独立に評価を行い、</p>
        <p class="indent">評価値の高い順に取り込んでいくことで解を得るという方法のことである。</p>
        <p class="indent">この手法で得られる解は最適解である保証はない。</p>
        <p>\(\Downarrow\)</p>
        <p>そこでLasso登場。</p>
        <!--<p>より原理的な手段。さらに凸最適化問題を解く。</p>-->
        <p>ここでは一般的な線形回帰モデルに対してLassoを用いた問題を考える。</p>
        <div class="border">
          <h4>二乗誤差損失のLasso回帰</h4>
          <p>目的関数</p>
          <p>\(\min_{\beta_0\leq\mathbb{R}, \bf{\beta}\leq\mathbb{R}^{p}}
            \frac{1}{n}\sum_{i=1}^{n}(y_i - \beta_0 - \beta^{T}\bf{x}_{\rm{i}}\rm{)^2}
            subject\ to\ ||\beta||_1 \leq t\ \ \ \ (16.1)\)
          </p>
          <p> </p>
        　<p>ここで、\(||\beta||_1\)はl1ノルムである。</p>
          <p>よって、損失関数も制約条件も\(\beta\)について凸であるので、</p>
          <p>この問題は凸最適化問題である。</p>
          <!--<p>凸であるとは、</p>
          <p>極小値が存在するなら、それは大域的最小値である。</p>-->
        </div>

        <p>この制約部分は、係数\(\beta\)が0になることを強いる。</p>
        <p>また、過学習を防ぐ。</p>

        <p>このLasso回帰の親戚にRidge回帰というものがある。</p>

        <div class="border">
          <h4>二乗誤差損失のRidge回帰</h4>
          <p>目的関数</p>
          <p>\(\min_{\beta_0\leq\mathbb{R}, \bf{\beta}\leq\mathbb{R}^{p}}
            \frac{1}{n}\sum_{i=1}^{n}(y_i - \beta_0 - \beta^{T}\bf{x}_{\rm{i}}\rm{)^2}
            subject\ to\ ||\beta||_2 \leq t\)
          </p>
          <p>\(\Downarrow\)</p>
          <p>制約条件だけが変わった。</p>
        </div>

        <p>Lasso回帰もRidge回帰もパラメータ縮小推定法である。</p>
        <p>縮小推定法とは、</p>
        <p>推定に関係ない説明変数に対応するパラメータの影響を縮小する。</p>
        <h4>\(\ \)(Lasso回帰とRidge回帰の)違い</h4>
        <p>Lassoでは係数\(\beta\)の多くが0となるが、</p>
        <p>Ridge回帰では全ての係数が非0となることだ。</p>
        <p>\(\Downarrow\)</p>
        <p>これより、Lasso回帰は縮小推定の手法でもあり、変数選択の手法でもある。</p>
        <p>しかし、Ridge回帰はただの縮小推定の手法である。</p>

        <h4>\(\ \)図16.4</h4>
        <p>上のことを表す図となっている。</p>
        <div align="center">
          <figure id="fig4">
            <img src="fig4.png" width="300" height="200">
            <figcaption>図16.4</figcaption>
          </figure>
        </div>
        <p>赤線は二乗誤差損失の等高線である。</p>
        <p>青の領域は制約条件を満たす領域である。</p>
        <p>また、Lassoでは全ての変数を同じ単位で扱うので標準化が必要である。</p>

        <h4>\(\ \)図16.5</h4>
        <p>spamデータを用いたLasso回帰のregularization pathである。</p>
        <div align="center">
          <figure id="fig5">
            <img src="fig5.png" width="300" height="200">
            <figcaption>図16.5</figcaption>
          </figure>
        </div>
        <p>図の上の数字が制約条件のtを表す。</p>
        <p>横軸は決定係数、縦軸は係数\(\beta\)の大きさを表す。</p>
        <p>灰色の縦線は係数が非0となった瞬間を表す。</p>
        <p>図16.1と比較してわかるように、forward stepwise法より、</p>
        <p>滑らかに係数が変化する。</p>
        <!--<p>16.4節で詳しく説明するが、係数\(\beta\)は制約条件のtに対して、</p>
        <p>区分線形であるので、このpathを正確にかける。</p>-->

        <h4>\(\ \)図16.6</h4>
        <p>spamデータを用いてforward stepwise法とLasso回帰の</p>
        <p>性能比較を表した図である。</p>
        <div align="center">
          <figure id="fig6">
            <img src="fig6.png" width="300" height="200">
            <figcaption>図16.6</figcaption>
          </figure>
        </div>

        <p>その結果、forward stepwise法とLasso回帰はほぼ同じ性能を発揮するが、</p>
        <p>Lassoの方が多くのステップが必要である。</p>
        <!-- 縮小推定のために -->

        <p>最後に、ロジスティック回帰の目的関数を下に示す。</p>

        <div class="border">
          <h4>Lassoロジスティック回帰(or尤度ベースの線形モデル)</h4>
          <p>目的関数</p>
          <!--<p>罰則付き最尤法を用いる。</p>-->
          <p>\(\min_{\beta_0\leq\mathbb{R}, \beta\leq\mathbb{R}^{p}}
            \frac{1}{n}\sum_{i=1}^{n} L(y_i, \beta_0 + \beta^{T}\bf{x}_i\rm{)^2}
            subject\ to\ ||\beta||_1 \leq t\ \ \ \ (16.2)\)
          </p>
          <p>ここでLは負の対数尤度関数である。</p>
        </div>

      </div>

      <div class="contexts">
        <h2 id="section3">3. Fitting Lasso Models</h2>
        <p>上のLassoモデルの目的関数を解く。</p>
        <p>目的関数(16.1),(16.2)は微分可能でβについて凸で、</p>
        <p>制約もβについて凸である。</p>
        <p>式(16.1)をラグランジュ形式で書き直すと、</p>
        <div align="center">
          <p>\(\min_{\beta \in \mathbb{R}^{p}} \frac{1}{2n}
            ||\bf{y} - \bf{X}\beta\rm{||^2} + \lambda||\beta||_1\ \ \ \rm{(16.3)}\)
          </p>
        </div>
        <p>となる。</p>
        <p>yとXは標準化で平均0より切片はなくした。</p>
        <!--<p>ここでの解は\(t=||β||_1\)に相当する。</p>-->
        <p>\(\lambda\)が大きいことは制約条件のtが小さいことに相当する。</p>
        <p>λ=0は制約条件なしの最小二乗法となる。</p>
        <p>式(16.3)の解は以下の劣勾配条件を満たす。</p>
        <p>(l1正則化項は微分不可能である)</p>
        <p>(そのような微分不可能な関数に微分の概念を拡張したものが劣微分である。)</p>
        <div align="center">
          <p>\( -\frac{1}{n}<\bf{x}_j, \bf{y}-\bf{X}\hat{\beta}\rm{>}
            + \lambda s_j = 0,\ (j=1,\cdots,p)\ \ \ (16.4)\)
          </p>
        </div>
        <p>ここで、\(s_j\)は、</p>
        <p>\(\hat{\beta}_j \not= 0\)なら、\(s_j = \pm 1\)で、</p>
        <p>\(\hat{\beta}_j = 0\)なら、\(s_j \in [-1,1]\)である。</p>
        <p>また、\(<,>\)は内積を表す。</p>
        <p>以上から、Lassoの解の性質がわかる。</p>

        <div class="border">
          <h4>Lassoの解βの性質</h4>
          <p>1. active setの説明変数(対応する係数が非0)\(\bf{x}_{\rm{j}}\)ついて以下が成り立つ。</p>
          <div align="center">
            <p>\( \frac{1}{n}|<\bf{x}_{\rm{j}}, \bf{y} - \bf{X}\hat{\beta}\rm{>|} = \rm{\lambda} \)</p>
          </div>
          <p>2. active setに含まれない説明変数(対応する係数が0)\(\bf{x}_{\rm{k}}\)について</p>
          <p>\(\ \ \ \ \ \)以下が成り立つ。</p>
          <div align="center">
            <p>\( \frac{1}{n}|<\bf{x}_{\rm{k}}, \bf{y} - \bf{X}\hat{\beta}\rm{>|} \leq \rm{\lambda} \)</p>
          </div>
        </div>
        <p>これらの性質は解の計算コストに大きな影響を与える。</p>
        <p>\(\Downarrow\)</p>
        <p>ある\(\lambda\)に対して、\(\hat{\beta}(\lambda_1)\)が決まったとして、</p>
        <p>\(\lambda_1\)より少し小さい\(\lambda_2\)を考える。</p>
        <p>\([\lambda_2,\lambda_1]\)の範囲で、active setが変わらないなら</p>
        <p>係数\(\hat{\beta}(\lambda)\)は\([\lambda_2, \lambda_1]\)の区間で線形である。</p>
        <p>active setをAとすると</p>
        <div align="center">
          <p>\(\bf{X}_{\rm{A}}^{\rm{T}}(\bf{y} - \bf{X}\hat{\beta}(\lambda_1)) = \rm{n}\bf{s}_A \lambda_1\)</p>
          <p>\(\bf{X}_{\rm{A}}^{\rm{T}}(\bf{y} - \bf{X}\hat{\beta}(\lambda_2)) = \rm{n}\bf{s}_A \lambda_2\)</p>
        </div>
        <p>が成り立つ。</p>
        <p>これを用いると、</p>
        <div align="center">
          <p>\(\bf{X}_{A}^{T}X(\hat{\beta}(\lambda_2) - \hat{\beta}(\lambda_1))
            = \rm{n}(\lambda_1 - \lambda_2)\bf{s_A}\)
          </p>
        </div>
        <p>が得られる。</p>
        <p>また、アクティブセットに含まれない説明変数に対応する係数\(\beta\)は0より</p>
        <div align="center">
          <p>\((\hat{\beta}_A (\lambda_2) - \hat{\beta}_A (\lambda_1))
            = n(\lambda_1 - \lambda_2)(\bf{X}_{\rm{A}}^{\rm{T}} X_{\rm{A}})^{-1}\bf{s}_{\rm{A}}\ \ \ \rm{(16.5)}\)
          </p>
        </div>
        <p>が得られる。</p>
        <p>以上より、Lasso回帰の係数\(\beta\)は\(\lambda\)に対して連続で区分線形である。</p>
        <p>ただし、active setが変わるときや係数の符号が変わるとき結び目ができる。</p>

        <p>また、もう一つの結果として</p>
        <p>\(\hat{\beta}(\lambda_{max})=0\)を満たす\(\lambda\)の最小値\(\lambda_{max}\)</p>
        <p>を簡単に決定できる。</p>
        <p>式(16.4)を用いて、</p>
        <div align="center">
          <p>\(\lambda_{max} = \max_{j} \frac{1}{n}|<\bf{x}_j, \bf{y}\rm{>|}\)</p>
        </div>
        <p>と求まる。</p>
        <p></p>
        <h4>\(\ \)イメージ図</h4>
        <div align="center">
          <figure id="solution">
            <img src="solution.jpg" width="300" height="200">
            <figcaption>solution path</figcaption>
          </figure>
        </div>

        <!--<p>これらの2つの事実とプラスαが二乗誤差損失のLassoの正確なsolution path</p>
        <p>を求めることを可能にする。</p>
        <p>これは次の節で説明する。</p>-->

      </div>

      <div class="contexts">
        <h2 id="section4">4. Least-Angle Regression</h2>
        <p>Lassoでは解が解析的に求められない。</p>
        <p>そこでこの節では、Lassoの解の求め方を説明する。</p>
        <p>以降、解の推定とか係数\(\beta\)の推定とかは全て、上のsolution pathの推定。</p>

        <p>上の節で、Lasso回帰で得られる係数\(\beta\)が区分線形であることがわかった。</p>
        <!--<p>線形回帰の解を疎に求めるアルゴリズムとして2004年に提案されました</p>-->
        <p>ここで、</p>
        <div align="center">
          <p>\(\bf{r}(\lambda) = \bf{y} - \bf{X}\hat{\beta}(\lambda)\)</p>
          <p>\(c_j(\lambda) = \frac{1}{n}|<\bf{x}_{\rm{j}}, \bf{r}(\lambda)\rm{>|}\)</p>
        </div>
        <p>とおく。</p>
        <!--<p>これらも区分線形である。</p>-->
        <p>係数\(\beta\)の性質より、</p>
        <p>\(j\in A\)なら、\(c_j(\lambda)=\lambda\)</p>
        <p>\(j \notin A\)なら、\(c_j(\lambda)<\lambda\)を満たす。</p>
        <p>\(\Downarrow\)</p>
        <p>この性質を利用してLassoの解を導出するLARアルゴリズム</p>


        <div class="border">
          <h4>アルゴリズム16.3 Least Angle Regression</h4>
          <p>1. ステップ1</p>
          <p class="indent">説明変数を標準化する。</p>
          <p class="indent">また、残差と係数\(\beta\)を</p>
          <div align="center">
            <p>\(\bf{r}_0 = \bf{y} - \bar{\bf{y}}\)</p>
            <p>\(\beta_0 =(\beta_1, \cdots, \beta_p)=\bf{0}\)</p>
          </div>
          <p class="indent">から始める。</p>
          <p>2. ステップ2</p>
          <p class="indent">最も\(\bf{r}_0\)と相関する説明変数\(\bf{x}_{\rm{j}}\)を見つける。</p>
          <p class="indent">アクティブセットを\(A=\{j\}\)とし、</p>
          <p class="indent">またその変数を含む行列を\(\bf{X}_{\rm{A}}\)とおく。</p>
          <p class="indent">最後に\(\hat{\beta}(\lambda_{0})=0\)を満たす\(\lambda\)の最小値を\(\lambda_{0}\)とおく。</p>
          <p>3. ステップ3</p>
          <p class="indent">\(k=1,2,\cdots,K=\min(n-1,p)\)において</p>
          <p>3.1 (a)</p>
          <p class="indent">最小二乗方向を</p>
          <div align="center">
            <p>\(\delta = \frac{1}{n\lambda_{k-1}}(X_A^{T}X_A)^{-1}X_A^{T}r_{k-1}\)</p>
          </div>
          <p class="indent">で定義する(\(\delta\in\mathbb{R}^{|A|}\))。</p>
          <p class="indent">また、サイズがpのベクトルの\(\Delta\)を</p>
          <div align="center">
            <p>\(\Delta_A = \delta\)</p>
          </div>
          <p class="indent">で定義する。</p>
          <p class="indent">ここで\(\Delta\)はactive set Aに含まれる変数に対応する</p>
          <p class="indent">成分のみ値を持ち、他は0である。</p>
          <p>3.2 (b)</p>
          <p class="indent">係数\(\beta\)を\(\beta^{k-1}\)から\(\Delta\)方向に移動させる。</p>
          <!--<p>\(X_A\)の最小二乗方向に\(\beta\)を更新する。</p>-->
          <div align="center">
            <p>\(\beta(\lambda) = \beta^{k-1} + (\lambda_{k-1} - \lambda)\Delta\)</p>
          </div>
          <p class="indent">ここで、\(0<\lambda\leq\lambda_{k-1}\)とする。</p>
          <p class="indent">また、残差は</p>
          <div align="center">
            <p>\(\bf{r}(\lambda) = \bf{y} - \bf{X}\beta(\lambda)\)</p>
            <p>\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
              = \bf{y} - \bf{X}\rm{(\beta^{k-1}+(\lambda_{k-1} - \lambda)\Delta)}\)</p>
            <p>\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
              = \bf{r}_{\rm{k-1}} - (\lambda_{k-1} - \lambda)\bf{X}\rm{\Delta}\)</p>
            <p>\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
              = \bf{r}_{k-1} - (\lambda_{k-1} - \lambda)\bf{X}_A \delta\)</p>
          </div>
          <p class="indent">である。</p>
          <p>(c)</p>
          <p class="indent">\(\lambda\)を決定する。</p>
          <p class="indent">アクティブセットAに含まれない説明変数\(\bf{x}_{\rm{l}}\)に対して、</p>
          <div align="center">
            <p>\(\frac{1}{n}|<\bf{x}_{\rm{l}}, \bf{r}(\lambda)\rm{>|}=\lambda\)</p>
          </div>
          <p class="indent">を満たす\(\lambda\)の最大値を求める。(\(0<\lambda\leq \lambda_{k-1}\))</p>
          <p>(d)</p>
          <p class="indent">更新</p>
          <p class="indent">アクティブセットを\(A=A\cup l\)で更新</p>
          <p class="indent">係数\(\beta\)を\(\beta^k = \beta(\lambda_k) = \beta^{k-1} +
            (\lambda_{k-1} - \lambda_{k})\Delta\)で更新</p>
          <p class="indent">残差\(\bf{r}\)を\(\bf{r}_{\rm{k}} = \bf{y} - \bf{X}\beta^{\rm{k}}\)で更新</p>
          <p>4. ステップ4</p>
          <p class="indent">列\(\{ \lambda_k , \beta^k \}_0^K\)を返す。</p>
        </div>

        <!--<p>LARアルゴリズムはforward stepwise回帰の大衆的なver</p>-->
        <p>forward stepwise回帰では、適合度を最も改善する変数を特定する。</p>
        <p>それからactive setに含まれる全ての変数を用いて二乗誤差損失が</p>
        <p>最小になるように係数を変更する。</p>
        <p>LARアルゴリズムでは、アクティブセットAに含まれる</p>
        <p>説明変数の係数を二乗誤差損失が小さくなる方向に動かす。</p>
        <p>そして、アクティブセットAに含まれない説明変数の内積が追いつくと、</p>
        <p>その変数がアクティブセットに入れられ、またプロセスが続く。</p>

        <p>ステップ3(c)は内積の線形性のために効率的に行われる。</p>
        <!--<p>つまり、アクティブセットAに含まれない説明変数たちが</p>
        <p>いつ追いつくか正確にわかる。</p>-->

        <p>また、区分線形性のために\(\lambda_{k-1}\)と\(\lambda_k\)の間の係数の計算をしなくても</p>
        <p>正確に係数の変化がわかる。</p>

        <!--<p>LARの由来</p>-->

        <p>アルゴリズム16.3の計算上の主な負担は、ステップ3(a)での新しい方向の</p>
        <p>計算である。</p>
        <p>しかし、これもQR分解の標準更新を用いて簡単に行われる。</p>
        <!--<p>これより、全てのpathの計算は全ての変数を用いた1つの最小二乗fitと同じorderである。</p>-->

        <h4>\(\ \)図16.7</h4>
        <p>LARアルゴリズムのステップ間での共分散の減少を示す。</p>
        <p>共分散は変数と残差の共分散で以下のやつ。</p>
        <div align="center">
          <p>\(c_j(\lambda) = \frac{1}{n}|<\bf{x}_{\rm{j}}, \bf{r}(\lambda)\rm{>|}\)</p>
        </div>
        <p>この共分散の性質としては以下である。</p>
        <p>\(j\in A\)なら、\(c_j(\lambda)=\lambda\)</p>
        <p>\(j \notin A\)なら、\(c_j(\lambda)<\lambda\)を満たす。</p>
        <div align="center">
          <figure id="fig7">
            <img src="fig7.png" width="300" height="200">
            <figcaption>図16.7</figcaption>
          </figure>
        </div>
        <p>各変数がactive setに含まれるたびに、共分散は結ばれる。</p>
        <p>pathの終わりで、共分散は全て0になる。</p>
        <!--<p>なぜなら、pathの終わりは制約なしの最小二乗の解と同じだから。</p>-->
        <h3>\(\ \)補足</h3>
        <p>LARアルゴリズムは厳密にはLassoのpathではない。</p>
        <p>Lassoではactive setに含まれる変数がactive setから</p>
        <p>drop outすることがある。</p>
        <p>これは係数の大きさが非0から0になる時に起こる。</p>
        <!--<p>劣勾配方程式(16.4)はactive setの変数に対応する係数の符号が</p>
        <p>勾配の符号と一致することを示す。</p>-->
        <p>\(\Downarrow\)</p>
        <p>ステップ3(c)への簡単な変更でこの問題は解決する。</p>
        <h4>\(\ \)修正</h4>
        <p>非0の係数は次の説明変数がactive setに入る前に0を交差する場合、</p>
        <p>active set Aからその説明変数をdropする。</p>
        <p>そして、縮小されたactive setを使用して最小二乗方向\(\Delta\)を計算する。</p>
        <p>\(\Downarrow\)</p>
        <p>これは変数が強く相関している時に起こりやすい。</p>

        <h3 id="subsection41">4.1 Lasso and Degrees of Freedom</h3>
        <h4>図16.8</h4>
        <div align="center">
          <figure id="fig8">
            <img src="fig8.png" width="300" height="200">
            <figcaption>図16.8</figcaption>
          </figure>
        </div>
        <p>図16.8の左の図を見ると、forward stepwise回帰はLasso回帰より</p>
        <p>早くMSEを小さくする。</p>
        <p>12章のdfの共分散式を用いて、各ステップでfitting度合いを測る。</p>
        <p>ここでdfはdegree of freedom、自由度のことであり</p>
        <div align="center">
          <p>\(df = \frac{1}{\sigma^2}\sum_{i=1}^n cov(y_i , \hat{y}_i )\ \ \ \ (16.6)\)</p>
        </div>
        <p>で定義される。</p>
        <p>右の図ではspamデータでforward stepwise回帰とLassoのdfを推定した</p>
        <p>シミュレーション結果を示す。</p>



        <p>図16.8の右図の茶色のプロットからわかるように、</p>
        <p>LARアルゴリズムでは各ステップで１つdfが増える。</p>
        <p>forward-stepwiseではより早い段階でより多くのdfが増える。</p>

        <p>X行列に関するいくつかの条件下では、</p>
        <p>dfが正確に1ステップで1つ増えることが言える。</p>
        <p>より一般的に言うと、Lassoでは、</p>
        <div align="center">
          <p>\(\hat{df}(\lambda) = |A(\lambda)|\)</p>
        </div>
        <p>と定義するなら、(\(|A(\lambda)|\)は\(\lambda\)でのactive setのサイズ)</p>
        <div align="center">
          <p>\(E[\hat{df}(\lambda)] = df(\lambda)\)</p>
        </div>
        <p>となる。</p>
        <p>言い換えると、active setのサイズはdfの不偏推定量である。</p>

        <p>forward stepwise変数選択法では探索でより多くのdfを費やす。</p>
        <p>Lassoは次の変数を探索するが、次の変数が入るまでは、</p>
        <p>完全に新しいモデルにはならない</p>
        <p>新しい変数が含まれると、1つ新しいdfが使われる。</p>

      </div>

      <div class="contexts">
        <h2 id="section5">5. Fitting Generalized Lasso Models</h2>
        <p>今までは、二乗誤差損失のLassoにだけ焦点を当て、</p>
        <p>係数の区分線形性を効率的な係数の推定に利用してきた。</p>
        <p>しかし、二乗誤差損失以外の損失関数ではこの区分線型性は生じない。</p>
        <p>よって潜在的に係数\(\beta\)の推定にコストがかかる。</p>
        <h4>\(\ \)例</h4>
        <p>ロジスティック回帰による2値分類問題を考える。</p>
        <p>目的関数は以下のようにかける。</p>
        <div align="center">
          <p>\(\min_{\beta_0\in\mathbb{R}, \beta\in\mathbb{R}^{p}}
            -[\frac{1}{n}\sum_{i=1}^{n} y_i \log \mu_i +
              (1-y_i)\log (1-\mu_i)] + \lambda ||\beta||_1\ \ \ \ (16.7)\)</p>
        </div>
        <p>ここで、\(y_i\)は0か1である。</p>
        <p>また、\(\mu_i\)は</p>
        <div align="center">
          <p>\(\mu_i = \frac{\exp(\beta_0 + \bf{x}_i^{T}\beta)}{1 + \exp(\beta_0 + \bf{x}_i^{T}\beta)}\ \ \ \ (16.8)\)</p>
        </div>
        <p>である。</p>

        <p>式(16.4)と同様に係数\(\beta\)が劣勾配条件</p>
        <div align="center">
          <p>\( \frac{1}{n}<\bf{x}_j , \bf{y} - \bf{\mu}> - \lambda \rm{s_j} = 0\ \ \ \rm{(j=1,\cdots,p)}\ \ \ \ \ (16.9) \)</p>
        </div>
        <p>を満たす。</p>
        <p>ここで、\(s_j\)は、</p>
        <p>\(\hat{\beta}_j \not= 0\)なら、\(s_j = \pm 1\)で、</p>
        <p>\(\hat{\beta}_j = 0\)なら、\(s_j \in [-1,1]\)である。</p>
        <p>また\(\mu^{T} = (\mu_1, \cdots, \mu_n)\)である。</p>
        <p>しかし、\(\mu_i\)の非線形性が係数\(\beta\)の性質として区分非線型性をもたらす。</p>
        <p>そこで、係数\(\beta\)を推定するために、\(\lambda\)に対する十分細かいグリッドを用意する。</p>

        <p>まず、\(\lambda\)の最大値を求める。</p>
        <div align="center">
          <p>\( \lambda_{\max} = \max_{j} |<\bf{x}_j, \bf{y} - \bar{\bf{y}}\bf{1}\rm{>|\ \ \ \ \ (16.10)} \)</p>
        </div>
        <p>これは\(\hat{\beta}=0\)を満たす最小の\(\lambda\)である。</p>

        <p>そして、\(\lambda_{\max}\)を\(\lambda_1\)として、間隔\(\epsilon \lambda_{\max}\)ごとに</p>
        <p>(\(\lambda_2 > \cdots > \lambda_{100}\))の\(\lambda\)に対して\(\beta\)を求めてく。</p>
        <p>ここで、\(\epsilon\)は0.001くらいの大きさである。</p>

        <p>この問題を効率的に解く手法として、path-wise coordinate descentがある。</p>

        <div class="border">
          <h4>path-wise coordinate descent(path-wise 座標降下)</h4>
          <p>1.</p>
          <p class="indent">各\(\lambda_k\)に対して、1つの\(\beta_j\)以外の係数は固定して、Lassoの問題を解く。</p>
          <p class="indent">推定値が安定するまで周りを回る。 </p>
          <p>2.</p>
          <p class="indent">\(\lambda_1\)から始める。ここでは全ての係数\(\beta\)が0である。</p>
          <p class="indent">\(\lambda_1,\cdots,\lambda_{100}\)に対する\(\beta\)の計算にwarm startsを用いる。</p>
          <p class="indent">warm startsは一連の\(\hat{\beta}(\lambda_k)\)に対して素晴らしい初期設定を与える。</p>
          <p>3.</p>
          <p class="indent">active setは\(\lambda\)が減るにつれてゆっくり変わる。</p>
          <p class="indent">active setを推測する計算上のhedges(制限?)は、</p>
          <p class="indent">特に効率的であることが証明されている。</p>
          <p class="indent">推測が正しいなら、収束するまで、active setの変数で</p>
          <p class="indent">座標降下を繰り返す。</p>
          <!--<p class="indent">全ての変数の一掃は直感を確認する。</p>-->
        </div>

        <p>Rのパッケージglmnetではproximal-Newton法を各\(\lambda_k\)用いる。</p>
        <div class="border">
          <h4>proximal-Newton法</h4>
          <p>1. ステップ1</p>
          <p class="indent">解\(\hat{\beta}(\lambda_k)\)の正確な推定で、</p>
          <p class="indent">対数尤度Lに対する重み付最小二乗(2次)近似を計算する。</p>
          <!--<p class="indent">これは通常のGLMのように動作応答と観測重みを生成する。</p>-->
          <p>2. ステップ2</p>
          <p class="indent">\(\lambda_k\)での重み付き最小二乗Lassoを座標降下で解く。</p>
          <p class="indent">そこで、warm startsとactive setの繰り返しを用いる。</p>
        </div>

        <p>以下に詳細を与える。</p>
        <!--<p>ここで、なぜこれらの手法が効率的か説明する。</p>-->

        <p>重み付き最小二乗問題</p>
        <div align="center">
          <p>\(\min_{\beta_j}
            \frac{1}{2n}\sum_{i=1}^{n} w_i(z_i - \beta_0 - \bf{x}_i^{T} \beta)^\rm{2} +
             \rm{\lambda ||\beta||_1\ \ \ \ (16.11)}\)</p>
        </div>
        <p>ここで、\(\beta_j\)を除いて全ての値が固定されている。</p>
        <p>また、</p>
        <div align="center">
          <p>\(r_i = z_i - \beta_0 - \sum_{l \not= j}x_{il}\beta_{l}\)</p>
        </div>
        <p>とおくと、式(16.11)は以下のように書き変えれる。</p>
        <div align="center">
          <p>\(\min_{\beta_j}
            \frac{1}{2n}\sum_{i=1}^{n} w_i(r_i - x_{ij}\beta_{j})^2 +
             \lambda ||\beta_j||_1\ \ \ \ (16.12)\)</p>
        </div>
        <p>この劣勾配方程式は</p>
        <div align="center">
          <p>\(\frac{1}{n}\sum_{i=1}^{n} w_i x_{ij}(r_i - x_{ij}\beta_{j}) -
             \lambda \rm{sign}(\beta_j) = 0\ \ \ \ (16.13)\)</p>
        </div>
        <p>となる。</p>
        <p>もし各変数が重み付き平均0,分散1に標準化されるなら</p>
        <p>解の簡単な形式が得られる。</p>
        <p>また、重みの和は1となる。</p>
        <p>この場合、2段階手法でこの問題を解く。</p>

        <div class="border">
          <p>1. ステップ1</p>
          <p class="indent">重み付き最小二乗係数を計算する。</p>
          <div align="center">
            <p>\( \tilde{\beta}_j = <\bf{x}_{\rm{j}}, \bf{r}\rm{>}_{\bf{w}} = \rm{\sum_{i=1}^{n} w_i x_{ij}}r_{i}\ \ (16.4)\)</p>
          </div>

          <p>2. ステップ2</p>
          <p class="indent">\(\hat{\beta}_j\)を生成するソフト閾値\(\tilde{\beta}_{j}\)について</p>
          <div align="center">
            <p>\(\hat{\beta}_{j} = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (|\tilde{\beta}_j |<\lambda)
              \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \)</p>
            <p>\(\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
              = \rm{sign}(\tilde{\beta}_j)(|\tilde{\beta}_j| - \lambda)\ \ \ (othewise)  \ \ \ \ (16.15)\)</p>
          </div>
        </div>

        <p>標準化がなくても解はほぼ一緒だが、直感的でない。</p>

        <!--<p>各勾配降下更新は、本質的に内積と、ソフト閾値処理が必要である。</p>
        <p>これは、\(x_{ij}\)が疎行列形式で保存される場合に特に役立つ。</p>
        <p>なぜなら、内積は非0の時だけ計算が行われる。</p>-->
        <p>ステップ前(\(\lambda_{k-1}\))に係数がの値が0のものがステップ後(\(\lambda_k\))も0のままなら、</p>
        <p>係数がただ動く。そうでなければモデルは更新される。</p>
        <p>また、\(\lambda_k\)での係数\(\beta\)に対して、</p>
        <div align="center">
          <p>\(|<\bf{x}_{\rm{j}}, \bf{r}>_{\bf{w}}\rm{| = \lambda_k}\)</p>
        </div>
        <p>が成り立っている(\(j \in A\))。</p>
        <p>そして、\(\lambda_{k+1}(< \lambda_k)\)において</p>
        <div align="center">
          <p>\(|<\bf{x}_{\rm{j}}, \bf{r}>_{\bf{w}}\rm{| \geq \lambda_{k+1}}\)</p>
        </div>
        <p>が成り立つ\(\bf{x}_{\rm{j}}\)が\(\lambda=\lambda_{k+1}\)でactive setに含まれる候補となる。</p>

        <h4>\(\ \)elastic net</h4>
        <p>LassoとRidgeの中間的なやつ。</p>
        <p>正則化項が以下で定義される。</p>
        <div align="center">
          <p>\(P_{\alpha}(\beta) = \frac{1}{2}(1-\alpha)||\beta||_2^2 + \alpha||\beta||_1\ \ \ \ (16.16)\)</p>
        </div>
        <p>説明変数が強く相関しているとき、Lassoの性能は悪くなる。</p>
        <p>elastic netではRidge部分のおかげで、改善される。</p>
        <p>この場合も、座標効果の更新は、式(16.5)と同様に簡単。</p>
        <div align="center">
          <p>\(\hat{\beta}_j = 0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
            (|\tilde{\beta}| < \alpha\lambda)\ \ \ \ \ \ \ \ \)</p>
          <p>\(= \frac{\rm{sign}(\tilde{\beta}_{j})(|\tilde{\beta}_j| - \alpha\lambda)}
            {1 + (1-\alpha)\lambda}\ \ (otherwise)\)</p>
        </div>


        <!--<h4>\(\ \)図16.9</h4>
        <p>Lassoとforward stepwiseロジスティク回帰をspamデータで比較する。</p>
        <div align="center">
          <figure id="fig9">
            <img src="fig9.png" width="300" height="200">
            <figcaption>図16.9</figcaption>
          </figure>
        </div>
        <p>Lassoでの最小誤分類エラーは0.057に対して、forward stepwiseは0.064である。</p>-->
        <h4>\(\ \)まとめ</h4>
        <p>wide dataを用いて適切な数の変数選択するとき、</p>
        <p>forward stepwise変数選択法では長い時間がかかる。</p>
        <p>なぜなら、1ステップでactive setに1つの変数しか入らないから。</p>
        <p>Lassoでは各ステップ(\(\lambda_k\))で、多くの変数がactive setに入る。</p>
        <p>よって、Lassoの方が係数の推定にかかる時間が短い。</p>
        <p>これより、Lassoはwide dataに対して魅力的である。</p>

      </div>

      <div class="contexts">
        <h2 id="section6">6. Post-Selection Inference for the Lasso</h2>
        <p>この節では、</p>
        <p>推論には注意を向けず、解釈可能なモデルの構築について説明する。</p>
        <!--<p>確かに推論は適応的に選択されたモデルにとっては困難である。</p>-->
        <p>\(\Downarrow\)</p>
        <p>ある\(\lambda\)でLasso回帰モデルをfitしたと仮定する。</p>
        <p>active setのサイズはkとする。</p>
        <p>ここで疑問が生じる。</p>
        <p>これらの選択された変数にp値を割り当てられるか、</p>
        <p>また係数の信頼区間を生成できるかだ。</p>
        <p>p値とは</p>
        <p>統計量（確率変数）がデータから計算した統計量の値より極端な値を取る確率</p>
        <p>\(\Downarrow\)</p>
        <p>最近の盛んな研究活動により、これらの重要な疑問を解決した。</p>
        <p>ここで簡単なやつを説明する。</p>
        <p>より一般的な話は20章でやる。</p>
        <p>\(\Downarrow\)</p>
        <p>生じる疑問の1つのは</p>
        <p>1. p個全ての説明変数の集合を使用した</p>
        <p>\(\ \ \ \ \)集団回帰パラメータの推論に関心があるのか、</p>
        <p>2. サブセットAの説明変数の集合を使用した</p>
        <p>\(\ \ \ \ \)集団回帰パラメータの推論に関心があるのかである。</p>
        <div class="border">
          <h4>1つ目のケース</h4>
          <p class="indent">選択されたモデルの係数を完全な母集団係数ベクトルの効率的だが偏った推定値として</p>
          <p class="indent">見ることができると提案されている。</p>
          <p class="indent">もちろん、最初に選択されたより強力な変数については、よりシャープな推論が利用可能になります。</p>
          <h4>２つ目のケース</h4>
          <p>アイディアとしては変数選択とactive set自体を条件づけることだ。</p>
          <p>Aの変数のみの応答の制約なし(すなわち、lassoで収縮していない）回帰係数に対して条件付き推論を実行することである。</p>
          <p>二乗誤差損失のLassoの場合、active set内の変数を導く応答\(\bf{y}\)が凸超多面体を形成することがわかる。</p>
          <p>(もし係数の符号についても条件づけるなら、符号を無視することは超多面体の有限結合を導く。)</p>
          <p>これは、細かいガウス条件付けの議論とともに、関心のあるパラメータのためのガウス分布およびt-分布を切り捨てることにつながります。</p>
        </div>

        <h4>図16.10</h4>
        <p>HIVの研究でLassoを用いた変数選択の結果を示す。</p>
        <div align="center">
          <figure id="fig10">
            <img src="fig10.png" width="300" height="200">
            <figcaption>図16.10</figcaption>
          </figure>
        </div>
        <p>結果Yは、HIV-1治療に対する抵抗性の尺度であり、</p>
        <p>30の説明変数は突然変異が特定のゲノム部位で生じたかの指標である。</p>
        <p>10-fold cvを用いたLasso回帰は\(\lambda=0.003\)で7つの係数が非0であった。</p>
        <p>黒線：選択された変数の係数の95%信頼区間である。</p>
        <p>3つの変数が有意であり、2つはほぼ有意である。</p>
        <p>白線：同様の回帰における信頼区間で選択イベントに条件つけられる。</p>
        <p>一般に幅広く、変数s25だけが有意のままである。</p>

      </div>

      <div class="contexts bottom">
        <h2 id="section7">7. Connections and Extensions</h2>
        <p>Lassoモデルと他の有名な予測問題の手法との繋がりについて。</p>

        <h3 id="subsection71">7.1 Lasso Logistic Regression and the SVM</h3>
        <p>19.3節で、ridgeロジスティック回帰と線形SVMが共通することを示す。</p>
        <p>分離可能なデータなら、ridgeロジスティック回帰で\(\lambda\rightarrow 0\)とすれば</p>
        <p>SVMと一致する。</p>
        <p>さらに、これらの損失関数は多少似ている。</p>
        <p>より解も似ている。</p>

        <h3 id="subsection72">7.2 Lasso and Boosting</h3>
        <p>17章でboostingについてやる。</p>
        <p></p>

        <h3 id="subsection73">7.3 Extensions of the Lasso</h3>
        <p>1. Group Lasso</p>
        <p>2. Graphical Lasso</p>
        <p>3. 判別分析、正準相関分析</p>
        <p>4. 核ノルム</p>

      </div>

    </div>

    <div class="container markdown-left">
      <p>線形回帰モデル</p>
      <p class="small">\(y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} \)</p>
      <p>サンプル : \(i=1,\cdots,n\)</p>
      <p>説明変数 : \(x_{i1},\cdots,x_{ip}\)</p>
    </div>
    <script type="text/javascript" src=“main.js”></script>
  </body>
</html>
